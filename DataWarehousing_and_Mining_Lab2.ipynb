{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-BYtGL1sBGRvm_KvVQ0rlb4nTs9K5uRz",
      "authorship_tag": "ABX9TyMOIkv1/ycFrk65TA1VDL2r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suyeshrimal/DW-and-DM-Lab/blob/main/DataWarehousing_and_Mining_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
        "import time\n",
        "\n",
        "file_paths = {\n",
        "    'space.txt': '/content/drive/MyDrive/DataSets/space.txt',\n",
        "    'sports.txt': '/content/drive/MyDrive/DataSets/sports.txt'\n",
        "}\n",
        "min_support = 0.15\n",
        "min_confidence = 0.7\n",
        "\n",
        "def get_support(itemset, transactions):\n",
        "    count = sum(1 for tx in transactions if itemset.issubset(set(tx)))\n",
        "    return count / len(transactions)\n",
        "\n",
        "def apriori(transactions, min_support):\n",
        "    total_tx = len(transactions)\n",
        "    item_counts = defaultdict(int)\n",
        "\n",
        "    for tx in transactions:\n",
        "        for item in tx:\n",
        "            item_counts[frozenset([item])] += 1\n",
        "\n",
        "    frequent_itemsets = {item: count for item, count in item_counts.items() if count / total_tx >= min_support}\n",
        "    all_frequent = frequent_itemsets.copy()\n",
        "    current_freq = list(frequent_itemsets.keys())\n",
        "    k = 2\n",
        "\n",
        "    while current_freq:\n",
        "        candidates = set()\n",
        "        for i in range(len(current_freq)):\n",
        "            for j in range(i + 1, len(current_freq)):\n",
        "                union = current_freq[i] | current_freq[j]\n",
        "                if len(union) == k:\n",
        "                    candidates.add(union)\n",
        "\n",
        "        candidate_counts = defaultdict(int)\n",
        "        for tx in transactions:\n",
        "            tx_set = set(tx)\n",
        "            for candidate in candidates:\n",
        "                if candidate.issubset(tx_set):\n",
        "                    candidate_counts[candidate] += 1\n",
        "\n",
        "        current_freq = [item for item in candidate_counts if candidate_counts[item] / total_tx >= min_support]\n",
        "        all_frequent.update({item: candidate_counts[item] for item in current_freq})\n",
        "        k += 1\n",
        "\n",
        "    return all_frequent\n",
        "\n",
        "def generate_rules(frequent_itemsets, transactions, min_confidence):\n",
        "    total_tx = len(transactions)\n",
        "    rules = []\n",
        "    for itemset in frequent_itemsets:\n",
        "        if len(itemset) < 2:\n",
        "            continue\n",
        "        support_itemset = frequent_itemsets[itemset] / total_tx\n",
        "        for i in range(1, len(itemset)):\n",
        "            for antecedent in combinations(itemset, i):\n",
        "                antecedent = frozenset(antecedent)\n",
        "                consequent = itemset - antecedent\n",
        "                support_ante = get_support(antecedent, transactions)\n",
        "                support_cons = get_support(consequent, transactions)\n",
        "                confidence = support_itemset / support_ante\n",
        "                lift = confidence / support_cons\n",
        "                if confidence >= min_confidence:\n",
        "                    rules.append({\n",
        "                        'antecedents': set(antecedent),\n",
        "                        'consequents': set(consequent),\n",
        "                        'support': round(support_itemset, 2),\n",
        "                        'confidence': round(confidence, 2),\n",
        "                        'lift': round(lift, 2)\n",
        "                    })\n",
        "    return rules\n",
        "\n",
        "for name, path in file_paths.items():\n",
        "    print(f\"\\n===== Processing {name} =====\")\n",
        "\n",
        "    transactions = []\n",
        "    with open(path, 'r') as file:\n",
        "        next(file)\n",
        "        for line in file:\n",
        "            parts = line.strip().split(',')\n",
        "            transactions.append([item.strip() for item in parts[1:] if item.strip()])\n",
        "\n",
        "    frequent_itemsets_raw = apriori(transactions, min_support)\n",
        "    rules = generate_rules(frequent_itemsets_raw, transactions, min_confidence)\n",
        "\n",
        "    total_tx = len(transactions)\n",
        "    frequent_itemsets_df = pd.DataFrame([{\n",
        "        'itemsets': set(item),\n",
        "        'support': round(count / total_tx, 2)\n",
        "    } for item, count in frequent_itemsets_raw.items()])\n",
        "\n",
        "    rules_df = pd.DataFrame(rules)\n",
        "\n",
        "    print(\"\\nFrequent Itemsets:\\n\", frequent_itemsets_df)\n",
        "\n",
        "    if not rules_df.empty:\n",
        "        print(\"\\nAssociation Rules:\\n\", rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "    else:\n",
        "        print(\"\\nNo association rules found with confidence ≥\", min_confidence)\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df_fp = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "fp_itemsets = fpgrowth(df_fp, min_support=min_support, use_colnames=True)\n",
        "\n",
        "fp_rules = association_rules(fp_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
        "\n",
        "print(\"\\nFP-Growth Frequent Itemsets:\\n\", fp_itemsets)\n",
        "if not fp_rules.empty:\n",
        "    print(\"\\nFP-Growth Association Rules:\\n\", fp_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "else:\n",
        "    print(\"\\nNo association rules found using FP-Growth with confidence ≥\", min_confidence)\n",
        "\n",
        "\n",
        "start_apriori = time.time()\n",
        "frequent_itemsets_raw = apriori(transactions, min_support)\n",
        "rules = generate_rules(frequent_itemsets_raw, transactions, min_confidence)\n",
        "end_apriori = time.time()\n",
        "\n",
        "start_fp = time.time()\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df_fp = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "fp_itemsets = fpgrowth(df_fp, min_support=min_support, use_colnames=True)\n",
        "fp_rules = association_rules(fp_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
        "end_fp = time.time()\n",
        "\n",
        "print(f\"\\nExecution Time (Apriori): {round(end_apriori - start_apriori, 4)} seconds\")\n",
        "print(f\"Execution Time (FP-Growth): {round(end_fp - start_fp, 4)} seconds\")\n",
        "\n",
        "\n",
        "print(\"\\n=== Comparison Summary ===\")\n",
        "print(f\"Apriori generated {len(rules)} rules\")\n",
        "print(f\"FP-Growth generated {len(fp_rules)} rules\")\n",
        "if (end_apriori - start_apriori) > (end_fp - start_fp):\n",
        "    print(\"FP-Growth is faster than Apriori.\")\n",
        "else:\n",
        "    print(\"Apriori is faster than FP-Growth.\")\n",
        "print(\"Both algorithms generated similar types of association rules, but FP-Growth is generally more efficient for large datasets.\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_D4EqsaaDe1",
        "outputId": "7c43d936-ab83-4223-ef7f-85fd6213a405"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Processing space.txt =====\n",
            "\n",
            "Frequent Itemsets:\n",
            "                      itemsets  support\n",
            "0               {Robotic Arm}     0.33\n",
            "1              {Food Packets}     0.39\n",
            "2              {Sleeping Bag}     0.31\n",
            "3                 {Treadmill}     0.27\n",
            "4                {Space Suit}     0.31\n",
            "5                {3D Printer}     0.27\n",
            "6  {Carbon Dioxide Scrubbers}     0.24\n",
            "\n",
            "No association rules found with confidence ≥ 0.7\n",
            "\n",
            "===== Processing sports.txt =====\n",
            "\n",
            "Frequent Itemsets:\n",
            "          itemsets  support\n",
            "0      {football}     0.43\n",
            "1  {cricket ball}     0.35\n",
            "2        {gloves}     0.35\n",
            "3   {cricket bat}     0.39\n",
            "4         {juice}     0.41\n",
            "5  {water bottle}     0.27\n",
            "6     {ice cream}     0.25\n",
            "\n",
            "No association rules found with confidence ≥ 0.7\n",
            "\n",
            "FP-Growth Frequent Itemsets:\n",
            "     support        itemsets\n",
            "0  0.431373      (football)\n",
            "1  0.352941        (gloves)\n",
            "2  0.352941  (cricket ball)\n",
            "3  0.411765         (juice)\n",
            "4  0.392157   (cricket bat)\n",
            "5  0.274510  (water bottle)\n",
            "6  0.254902     (ice cream)\n",
            "\n",
            "No association rules found using FP-Growth with confidence ≥ 0.7\n",
            "\n",
            "Execution Time (Apriori): 0.0003 seconds\n",
            "Execution Time (FP-Growth): 0.0046 seconds\n",
            "\n",
            "=== Comparison Summary ===\n",
            "Apriori generated 0 rules\n",
            "FP-Growth generated 0 rules\n",
            "Apriori is faster than FP-Growth.\n",
            "Both algorithms generated similar types of association rules, but FP-Growth is generally more efficient for large datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EbR_47RmaDcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H_U3c3iMaDZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PEGCyA1WaDXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5t2BX5qEaDVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oRjY2uulaDSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p49XlcvJaDQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Z-zyTu4aDN1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}